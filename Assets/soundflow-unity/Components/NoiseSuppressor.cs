using System;
using System.Buffers;
using System.Collections.Generic;
using System.Runtime.InteropServices;
using SoundFlow.Interfaces;


namespace SoundFlow.Extensions.WebRtc.Apm.Components
{
    /// <summary>
    /// Delegate for handling processed audio chunks.
    /// </summary>
    /// <param name="processedChunk">A ReadOnlyMemory of float representing the processed audio chunk.</param>
    public delegate void ProcessedAudioChunkHandler(ReadOnlyMemory<float> processedChunk);

    /// <summary>
    /// Processes audio from an ISoundDataProvider using WebRTC Noise Suppression
    /// and provides the cleaned audio either as a complete raw float array or
    /// chunk by chunk via an event. This component is designed for offline/batch processing.
    /// </summary>
    public class NoiseSuppressor : IDisposable
    {
        private readonly ISoundDataProvider _dataProvider;
        private readonly AudioProcessingModule _apm;
        private readonly ApmConfig _apmConfig;
        private readonly StreamConfig _inputStreamConfig;
        private readonly StreamConfig _outputStreamConfig;

        private readonly int _numChannels;
        private readonly int _apmFrameSizePerChannel; // Samples per channel for a 10ms APM frame
        private const int BytesPerSample = sizeof(float);

        private readonly float[][] _deinterleavedInputApmFrame;
        private readonly float[][] _deinterleavedOutputApmFrame;

        private readonly nint[] _inputChannelPtrs;
        private readonly nint[] _outputChannelPtrs;
        private readonly nint _inputChannelArrayPtr;
        private readonly nint _outputChannelArrayPtr;
        private GCHandle _inputChannelArrayHandle;
        private GCHandle _outputChannelArrayHandle;

        private bool _isDisposed;

        /// <summary>
        /// Event raised when a chunk of audio has been processed.
        /// </summary>
        public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;

        /// <summary>
        /// Initializes a new instance of the <see cref="NoiseSuppressor"/> class.
        /// </summary>
        /// <param name="dataProvider">The audio data provider to process.</param>
        /// <param name="sampleRate">The sample rate of the audio from the dataProvider. Must be 8k, 16k, 32k, or 48k Hz.</param>
        /// <param name="numChannels">The number of channels in the audio from the dataProvider.</param>
        /// <param name="suppressionLevel">The desired level of noise suppression.</param>
        /// <param name="useMultichannelProcessing">If true and numChannels > 1, attempts to process channels independently. If false, channels may be downmixed by APM.</param>
        /// <exception cref="ArgumentNullException">Thrown if dataProvider is null.</exception>
        /// <exception cref="ArgumentException">Thrown if sampleRate or numChannels are invalid or unsupported by WebRTC APM.</exception>
        public NoiseSuppressor(
            ISoundDataProvider dataProvider,
            int sampleRate,
            int numChannels,
            NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High,
            bool useMultichannelProcessing = false)
        {
            _dataProvider = dataProvider ?? throw new ArgumentNullException(nameof(dataProvider));
            _numChannels = numChannels;

            if (sampleRate != 8000 && sampleRate != 16000 && sampleRate != 32000 && sampleRate != 48000)
                throw new ArgumentException($"Unsupported sample rate for WebRTC Audio Processing Module: {sampleRate} Hz. Must be 8k, 16k, 32k, or 48k.");
            if (_numChannels <= 0)
                throw new ArgumentException("Number of channels must be greater than 0.", nameof(numChannels));

            _apmFrameSizePerChannel = AudioProcessingModule.GetFrameSize(sampleRate);
            if (_apmFrameSizePerChannel == 0)
                throw new ArgumentException($"Could not determine APM frame size for sample rate {sampleRate} Hz.", nameof(sampleRate));

            var apmFrameSizeBytesPerChannel = _apmFrameSizePerChannel * BytesPerSample;

            _apm = new AudioProcessingModule();
            _apmConfig = new ApmConfig();

            // Configure APM specifically for Noise Suppression
            _apmConfig.SetNoiseSuppression(true, suppressionLevel);
            _apmConfig.SetEchoCanceller(false, false);
            _apmConfig.SetGainController1(false, GainControlMode.FixedDigital, 0, 0, false);
            _apmConfig.SetGainController2(false);
            _apmConfig.SetHighPassFilter(false);
            _apmConfig.SetPreAmplifier(false, 1.0f);

            // Configure pipeline for multi-channel or mono processing
            var multiChannelFlag = useMultichannelProcessing && _numChannels > 1;
            _apmConfig.SetPipeline(sampleRate, multiChannelFlag, multiChannelFlag, DownmixMethod.AverageChannels);

            var applyError = _apm.ApplyConfig(_apmConfig);
            if (applyError != ApmError.NoError)
            {
                _apm.Dispose();
                _apmConfig.Dispose();
                throw new InvalidOperationException($"Failed to apply APM config: {applyError}");
            }

            _inputStreamConfig = new StreamConfig(sampleRate, _numChannels);
            _outputStreamConfig = new StreamConfig(sampleRate, _numChannels);

            var initError = _apm.Initialize();
            if (initError != ApmError.NoError)
            {
                _apm.Dispose();
                _apmConfig.Dispose();
                _inputStreamConfig.Dispose();
                _outputStreamConfig.Dispose();
                throw new InvalidOperationException($"Failed to initialize APM: {initError}");
            }

            // Allocate managed and unmanaged buffers
            _deinterleavedInputApmFrame = new float[_numChannels][];
            _deinterleavedOutputApmFrame = new float[_numChannels][];
            _inputChannelPtrs = new nint[_numChannels];
            _outputChannelPtrs = new nint[_numChannels];

            try
            {
                for (var i = 0; i < _numChannels; i++)
                {
                    _deinterleavedInputApmFrame[i] = new float[_apmFrameSizePerChannel];
                    _deinterleavedOutputApmFrame[i] = new float[_apmFrameSizePerChannel];
                    _inputChannelPtrs[i] = Marshal.AllocHGlobal(apmFrameSizeBytesPerChannel);
                    _outputChannelPtrs[i] = Marshal.AllocHGlobal(apmFrameSizeBytesPerChannel);
                }

                _inputChannelArrayHandle = GCHandle.Alloc(_inputChannelPtrs, GCHandleType.Pinned);
                _inputChannelArrayPtr = _inputChannelArrayHandle.AddrOfPinnedObject();
                _outputChannelArrayHandle = GCHandle.Alloc(_outputChannelPtrs, GCHandleType.Pinned);
                _outputChannelArrayPtr = _outputChannelArrayHandle.AddrOfPinnedObject();
            }
            catch
            {
                Dispose();
                throw;
            }
        }

        /// <summary>
        /// Processes the entire audio stream from the provided data provider
        /// and returns the noise-suppressed audio as a single float array.
        /// This method is suitable for smaller audio files that can fit in memory.
        /// </summary>
        /// <returns>A float array containing the processed (noise-suppressed) audio data.</returns>
        public float[] ProcessAll()
        {
            //ObjectDisposedException.ThrowIf(_isDisposed, this);
            var processedAudioList = new List<float>();
            ProcessChunks(chunk => { processedAudioList.AddRange(chunk.ToArray()); });
            return processedAudioList.ToArray();
        }

        /// <summary>
        /// Processes the audio stream from the data provider in chunks.
        /// For each processed chunk, the <see cref="OnAudioChunkProcessed"/> event is raised.
        /// This method is suitable for processing large audio files without loading them entirely into memory.
        /// </summary>
        /// <param name="chunkHandler">An optional action to handle processed chunks directly, in addition to the event.</param>
        /// <exception cref="ObjectDisposedException">Thrown if the NoiseSuppressor has been disposed.</exception>
        /// <exception cref="InvalidOperationException">Thrown if the processing failed or if the APM configuration is invalid.</exception>
        public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null)
        {
            //ObjectDisposedException.ThrowIf(_isDisposed, this);

            var samplesPerApmFrameInterleaved = _apmFrameSizePerChannel * _numChannels;
            var providerReadBuffer = ArrayPool<float>.Shared.Rent(samplesPerApmFrameInterleaved);
            var processedFrameBuffer = ArrayPool<float>.Shared.Rent(samplesPerApmFrameInterleaved);

            try
            {
                int samplesActuallyReadFromProvider;
                do
                {
                    // Read a full APM frame's worth of interleaved samples from the provider
                    Array.Clear(providerReadBuffer, 0, samplesPerApmFrameInterleaved);
                    samplesActuallyReadFromProvider = _dataProvider.ReadBytes(
                        providerReadBuffer.AsSpan(0, samplesPerApmFrameInterleaved));

                    if (samplesActuallyReadFromProvider > 0)
                    {
                        var currentFrameSpan = providerReadBuffer.AsSpan(0, samplesPerApmFrameInterleaved);

                        // If last chunk is smaller than APM frame, pad with silence
                        // Note: The input providerReadBuffer is already samplesPerApmFrameInterleaved long
                        // so we just need to clear the part that wasn't filled by ReadBytes if it's a partial read.
                        if (samplesActuallyReadFromProvider < samplesPerApmFrameInterleaved)
                        {
                            currentFrameSpan[samplesActuallyReadFromProvider..].Clear();
                        }

                        Deinterleave(currentFrameSpan, _numChannels, _apmFrameSizePerChannel, _deinterleavedInputApmFrame);

                        for (var ch = 0; ch < _numChannels; ch++)
                            Marshal.Copy(_deinterleavedInputApmFrame[ch], 0, _inputChannelPtrs[ch], _apmFrameSizePerChannel);

                        var error = NativeMethods.webrtc_apm_process_stream(
                            _apm.NativePtr,
                            _inputChannelArrayPtr,
                            _inputStreamConfig.NativePtr,
                            _outputStreamConfig.NativePtr,
                            _outputChannelArrayPtr);

                        if (error == ApmError.NoError)
                        {
                            for (var ch = 0; ch < _numChannels; ch++)
                                Marshal.Copy(_outputChannelPtrs[ch], _deinterleavedOutputApmFrame[ch], 0, _apmFrameSizePerChannel);

                            Interleave(_deinterleavedOutputApmFrame, _numChannels, _apmFrameSizePerChannel, processedFrameBuffer.AsSpan(0, samplesPerApmFrameInterleaved));

                            // Raise event/handler with the valid portion of the processed chunk
                            var validProcessedChunk = processedFrameBuffer.AsMemory(0, samplesActuallyReadFromProvider);
                            OnAudioChunkProcessed?.Invoke(validProcessedChunk);
                            chunkHandler?.Invoke(validProcessedChunk);
                        }
                        else
                        {
                            // On error, pass through the original chunk
                            var originalValidChunk = providerReadBuffer.AsMemory(0, samplesActuallyReadFromProvider);
                            OnAudioChunkProcessed?.Invoke(originalValidChunk);
                            chunkHandler?.Invoke(originalValidChunk);
                            Console.Error.WriteLine($"Noise suppression process failed: {error}. Passing through chunk.");
                            // Optionally throw or handle error differently
                        }
                    }
                } while (samplesActuallyReadFromProvider == samplesPerApmFrameInterleaved);
            }
            finally
            {
                ArrayPool<float>.Shared.Return(providerReadBuffer);
                ArrayPool<float>.Shared.Return(processedFrameBuffer);
            }
        }


        private static void Deinterleave(ReadOnlySpan<float> interleaved, int numChannels, int frameSizePerChannel, float[][] deinterleavedTarget)
        {
            for (var ch = 0; ch < numChannels; ch++)
            {
                for (var i = 0; i < frameSizePerChannel; i++)
                {
                    var idx = i * numChannels + ch;
                    deinterleavedTarget[ch][i] = idx < interleaved.Length ? interleaved[idx] : 0f;
                }
            }
        }

        private static void Interleave(float[][] deinterleaved, int numChannels, int frameSizePerChannel, Span<float> interleavedTarget)
        {
            for (var ch = 0; ch < numChannels; ch++)
            {
                for (var i = 0; i < frameSizePerChannel; i++)
                {
                    var idx = i * numChannels + ch;
                    if (idx < interleavedTarget.Length) interleavedTarget[idx] = deinterleaved[ch][i];
                }
            }
        }

        /// <summary>
        /// Disposes this NoiseSuppressor instance.
        /// </summary>
        public void Dispose()
        {
            if (!_isDisposed)
            {
                if (_inputChannelArrayHandle.IsAllocated)
                {
                    _inputChannelArrayHandle.Free();
                }
                if (_outputChannelArrayHandle.IsAllocated)
                {
                    _outputChannelArrayHandle.Free();
                }
                for (var i = 0; i < _numChannels; i++)
                {
                    if (_inputChannelPtrs[i] != IntPtr.Zero)
                    {
                        Marshal.FreeHGlobal(_inputChannelPtrs[i]);
                    }
                }
                for (var i = 0; i < _numChannels; i++)
                {
                    if (_outputChannelPtrs[i] != IntPtr.Zero)
                    {
                        Marshal.FreeHGlobal(_outputChannelPtrs[i]);
                    }
                }
                _apm.Dispose();
                _apmConfig.Dispose();
                _inputStreamConfig.Dispose();
                _outputStreamConfig.Dispose();

                _isDisposed = true;
                GC.SuppressFinalize(this);
            }
        }

        ~NoiseSuppressor()
        {
            Dispose();
        }
    }
}